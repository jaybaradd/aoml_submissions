{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":982,"sourceType":"datasetVersion","datasetId":483}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport re\nimport nltk\nimport spacy\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom textblob import TextBlob\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\npd.options.mode.chained_assignment = None\nfrom nltk.corpus import wordnet\nimport os\nimport shutil","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-02T11:25:22.785891Z","iopub.execute_input":"2025-04-02T11:25:22.786271Z","iopub.status.idle":"2025-04-02T11:25:22.792398Z","shell.execute_reply.started":"2025-04-02T11:25:22.786241Z","shell.execute_reply":"2025-04-02T11:25:22.791304Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"import os\ncustom_nltk_dir = \"/kaggle/working/nltk_data\"\n\n# Check if WordNet is actually downloaded\nprint(\"WordNet exists:\", os.path.exists(f\"{custom_nltk_dir}/corpora/wordnet\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T11:25:00.228716Z","iopub.execute_input":"2025-04-02T11:25:00.229079Z","iopub.status.idle":"2025-04-02T11:25:00.235335Z","shell.execute_reply.started":"2025-04-02T11:25:00.229051Z","shell.execute_reply":"2025-04-02T11:25:00.234270Z"}},"outputs":[{"name":"stdout","text":"WordNet exists: True\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"\ncustom_nltk_dir = \"/kaggle/working/nltk_data\"\nnltk.data.path.append(custom_nltk_dir)\n\n# Ensure the directory exists\nos.makedirs(custom_nltk_dir, exist_ok=True)\n\n# Download WordNet manually\nnltk.download('wordnet', download_dir=custom_nltk_dir)\nnltk.download('omw-1.4', download_dir=custom_nltk_dir)  # Optional for extended WordNet\n\n# Extract if necessary\nwordnet_zip_path = os.path.join(custom_nltk_dir, \"corpora/wordnet.zip\")\nwordnet_dir_path = os.path.join(custom_nltk_dir, \"corpora/wordnet\")\n\nif os.path.exists(wordnet_zip_path):\n    shutil.unpack_archive(wordnet_zip_path, os.path.join(custom_nltk_dir, \"corpora\"))\n    print(\"WordNet extracted successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T11:19:20.769869Z","iopub.execute_input":"2025-04-02T11:19:20.770224Z","iopub.status.idle":"2025-04-02T11:19:21.208674Z","shell.execute_reply.started":"2025-04-02T11:19:20.770196Z","shell.execute_reply":"2025-04-02T11:19:21.207680Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package wordnet to\n[nltk_data]     /kaggle/working/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to\n[nltk_data]     /kaggle/working/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\nWordNet extracted successfully!\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/sms-spam-collection-dataset/spam.csv',encoding=\"ISO-8859-1\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T11:22:32.861381Z","iopub.execute_input":"2025-04-02T11:22:32.861743Z","iopub.status.idle":"2025-04-02T11:22:32.888407Z","shell.execute_reply.started":"2025-04-02T11:22:32.861717Z","shell.execute_reply":"2025-04-02T11:22:32.887504Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"df.drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4'], axis=1, inplace=True)\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T11:22:33.636751Z","iopub.execute_input":"2025-04-02T11:22:33.637111Z","iopub.status.idle":"2025-04-02T11:22:33.647299Z","shell.execute_reply.started":"2025-04-02T11:22:33.637085Z","shell.execute_reply":"2025-04-02T11:22:33.646266Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"     v1                                                 v2\n0   ham  Go until jurong point, crazy.. Available only ...\n1   ham                      Ok lar... Joking wif u oni...\n2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n3   ham  U dun say so early hor... U c already then say...\n4   ham  Nah I don't think he goes to usf, he lives aro...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>v1</th>\n      <th>v2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ham</td>\n      <td>Go until jurong point, crazy.. Available only ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ham</td>\n      <td>Ok lar... Joking wif u oni...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>spam</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>U dun say so early hor... U c already then say...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>Nah I don't think he goes to usf, he lives aro...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"df['v2'] = df['v2'].str.lower()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T11:22:39.695221Z","iopub.execute_input":"2025-04-02T11:22:39.695560Z","iopub.status.idle":"2025-04-02T11:22:39.703654Z","shell.execute_reply.started":"2025-04-02T11:22:39.695536Z","shell.execute_reply":"2025-04-02T11:22:39.702434Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"PUNCT_TO_REMOVE = string.punctuation\ndef remove_punctuation(text):\n    \"\"\"custom function to remove the punctuation\"\"\"\n    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n\ndf['v2'] = df[\"v2\"].apply(lambda text: remove_punctuation(text))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T11:22:55.238483Z","iopub.execute_input":"2025-04-02T11:22:55.238848Z","iopub.status.idle":"2025-04-02T11:22:55.274396Z","shell.execute_reply.started":"2025-04-02T11:22:55.238822Z","shell.execute_reply":"2025-04-02T11:22:55.273194Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"STOPWORDS = set(stopwords.words('english'))\ndef remove_stopwords(text):\n    \"\"\"custom function to remove the stopwords\"\"\"\n    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n\ndf['v2_stopWordRem'] = df[\"v2\"].apply(lambda text: remove_stopwords(text))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T11:22:57.319787Z","iopub.execute_input":"2025-04-02T11:22:57.320157Z","iopub.status.idle":"2025-04-02T11:22:57.350899Z","shell.execute_reply.started":"2025-04-02T11:22:57.320128Z","shell.execute_reply":"2025-04-02T11:22:57.349559Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# for i in range(df.shape[0]):\n    # print([word for word in df[\"v2_stopWordRem\"][i].split()])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T06:00:55.715853Z","iopub.execute_input":"2025-02-23T06:00:55.716293Z","iopub.status.idle":"2025-02-23T06:00:55.720712Z","shell.execute_reply.started":"2025-02-23T06:00:55.716260Z","shell.execute_reply":"2025-02-23T06:00:55.719675Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# # corrected_text = str(TextBlob(text).correct())\n# # print(corrected_text)\n# # [word for word in df[\"v2_stopWordRem\"][i].split()]\n# def spelling_nazi(text):\n#     return \" \".join([str(TextBlob(word).correct()) for word in text.split()])\n\n# df['spell_checked_v2'] = df[\"v2_stopWordRem\"].apply(lambda text: spelling_nazi(text))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T06:03:54.298508Z","iopub.execute_input":"2025-02-23T06:03:54.298924Z","execution_failed":"2025-02-23T07:25:54.122Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"nltk.download('wordnet', download_dir=custom_nltk_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T11:24:16.371001Z","iopub.execute_input":"2025-04-02T11:24:16.371401Z","iopub.status.idle":"2025-04-02T11:24:16.409287Z","shell.execute_reply.started":"2025-04-02T11:24:16.371371Z","shell.execute_reply":"2025-04-02T11:24:16.408299Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package wordnet to\n[nltk_data]     /kaggle/working/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n","output_type":"stream"},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"lemmatizer = WordNetLemmatizer()\nwordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\ndef lemmatize_words(text):\n    pos_tagged_text = nltk.pos_tag(text.split())\n    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n\ndf[\"v2_lemmatized\"] = df[\"v2_stopWordRem\"].apply(lambda text: lemmatize_words(text))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T11:25:27.555289Z","iopub.execute_input":"2025-04-02T11:25:27.555694Z","iopub.status.idle":"2025-04-02T11:25:31.890190Z","shell.execute_reply.started":"2025-04-02T11:25:27.555657Z","shell.execute_reply":"2025-04-02T11:25:31.889214Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Initialize CountVectorizer\nvectorizer = CountVectorizer()\n\n# Transform text data into BoW features\nX_bow = vectorizer.fit_transform(df['v2_lemmatized'])\n\n# Convert to DataFrame\nbow_df = pd.DataFrame(X_bow.toarray(), columns=vectorizer.get_feature_names_out())\n\n# Merge BoW features with original DataFrame\ndf_bow = pd.concat([df, bow_df], axis=1)\n\n# Drop the original text column (optional)\ndf_bow.drop(columns=['v2_lemmatized','v2_stopWordRem','v2'], axis=1, inplace=True)\n\n# Display result\nprint(df_bow)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T11:26:15.926789Z","iopub.execute_input":"2025-04-02T11:26:15.927165Z","iopub.status.idle":"2025-04-02T11:26:17.190840Z","shell.execute_reply.started":"2025-04-02T11:26:15.927137Z","shell.execute_reply":"2025-04-02T11:26:17.189700Z"}},"outputs":[{"name":"stdout","text":"        v1  008704050406  0089my  0121  01223585236  01223585334  0125698789  \\\n0      ham             0       0     0            0            0           0   \n1      ham             0       0     0            0            0           0   \n2     spam             0       0     0            0            0           0   \n3      ham             0       0     0            0            0           0   \n4      ham             0       0     0            0            0           0   \n...    ...           ...     ...   ...          ...          ...         ...   \n5567  spam             0       0     0            0            0           0   \n5568   ham             0       0     0            0            0           0   \n5569   ham             0       0     0            0            0           0   \n5570   ham             0       0     0            0            0           0   \n5571   ham             0       0     0            0            0           0   \n\n      02  020603  0207  ...  ìï  ìïll  ûthanks  ûªm  ûªt  ûªve  ûï  ûïharry  \\\n0      0       0     0  ...   0     0        0    0    0     0   0        0   \n1      0       0     0  ...   0     0        0    0    0     0   0        0   \n2      0       0     0  ...   0     0        0    0    0     0   0        0   \n3      0       0     0  ...   0     0        0    0    0     0   0        0   \n4      0       0     0  ...   0     0        0    0    0     0   0        0   \n...   ..     ...   ...  ...  ..   ...      ...  ...  ...   ...  ..      ...   \n5567   0       0     0  ...   0     0        0    0    0     0   0        0   \n5568   0       0     0  ...   0     0        0    0    0     0   0        0   \n5569   0       0     0  ...   0     0        0    0    0     0   0        0   \n5570   0       0     0  ...   0     0        0    0    0     0   0        0   \n5571   0       0     0  ...   0     0        0    0    0     0   0        0   \n\n      ûò  ûówell  \n0      0       0  \n1      0       0  \n2      0       0  \n3      0       0  \n4      0       0  \n...   ..     ...  \n5567   0       0  \n5568   0       0  \n5569   0       0  \n5570   0       0  \n5571   0       0  \n\n[5572 rows x 8441 columns]\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nX = df_bow.drop(['v1'],axis=1)\ny = df['v1']\n\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y)\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n\n# # Train Naive Bayes (for classification)\nnb_model = MultinomialNB()\nnb_model.fit(X_train, y_train)\ny_pred_nb = nb_model.predict(X_test)\nacc_nb = accuracy_score(y_test, y_pred_nb)\n\n# Train Random Forest\nrf_model = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_model.fit(X_train, y_train)\ny_pred_rf = rf_model.predict(X_test)\nacc_rf = accuracy_score(y_test, y_pred_rf)\n\n# Train XGBoost\nxgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric=\"mlogloss\", random_state=42)\nxgb_model.fit(X_train, y_train)\ny_pred_xgb = xgb_model.predict(X_test)\nacc_xgb = accuracy_score(y_test, y_pred_xgb)\n\n# Print results\nprint(f\"Naive Bayes Accuracy: {acc_nb:.4f}\")\nprint(f\"Random Forest Accuracy: {acc_rf:.4f}\")\nprint(f\"XGBoost Accuracy: {acc_xgb:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T11:30:37.126303Z","iopub.execute_input":"2025-04-02T11:30:37.126663Z","iopub.status.idle":"2025-04-02T11:30:57.811615Z","shell.execute_reply.started":"2025-04-02T11:30:37.126637Z","shell.execute_reply":"2025-04-02T11:30:57.810460Z"}},"outputs":[{"name":"stdout","text":"Naive Bayes Accuracy: 0.9785\nRandom Forest Accuracy: 0.9749\nXGBoost Accuracy: 0.9713\n","output_type":"stream"}],"execution_count":41}]}